{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) # 为了导入父目录中的文件而进行的设定\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "# normalize是否正规化为0.0~1.0之间的数值，如果为false则输出0～255。\n",
    "# 输出各个数据的形状\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "5\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(type((x_train, t_train)))\n",
    "print(label) # 5\n",
    "print(img.shape) # (784,)\n",
    "img = img.reshape(28, 28) # 把图像的形状变成原来的尺寸 print(img.shape) # (28, 28)\n",
    "#img_show(img)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network.type:<class 'dict'>\n",
      "b2\n",
      "W1\n",
      "b1\n",
      "W2\n",
      "W3\n",
      "b3\n",
      "x的类型：<class 'numpy.ndarray'>，长度:10000,x[0]类型:<class 'numpy.ndarray'>,x[0]长度:784,x[0]形状:(784,)\n",
      "t的类型：<class 'numpy.ndarray'>，长度10000,t[0]类型:<class 'numpy.uint8'>\n",
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import sigmoid, softmax\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True,one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "def init_network():\n",
    "    with open(\"../ch03/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "        return network\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    return y\n",
    "x,t = get_data()\n",
    "network = init_network()\n",
    "print('network.type:'+str(type(network)))\n",
    "for k in network:\n",
    "    print(k)\n",
    "accuracy_cnt = 0\n",
    "print('x的类型：'+str(type(x))+'，长度:'+str(len(x))+',x[0]类型:'+str(type(x[0]))+',x[0]长度:'+str(len(x[0]))+',x[0]形状:'+str(x[0].shape))\n",
    "print('t的类型：'+str(type(t))+'，长度'+str(len(t))+',t[0]类型:'+str(type(t[0])))\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y) # 获取概率最高的元素的索引\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST数据集\n",
    "load_mnist()\n",
    "- normalize是否正规化为0.0~1.0之间的数值，如果为false则输出0～255。\n",
    "- flatten是否展开输入图像，如果设置为false，输出1x28x28的三维数组\n",
    "- one_hot_lable one-hot 表示是仅正确解标签为 1，其余 皆为 0 的数组，就像 [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "x的类型：<class 'numpy.ndarray'>，长度:10000,x[0]类型:<class 'numpy.ndarray'>,x[0]长度:784,x[0]形状:(784,)\n",
    "t的类型：<class 'numpy.ndarray'>，长度10000,t[0]类型:<class 'numpy.uint8'>\n",
    "network.type:<class 'dict'>\n",
    "### 神经网络的推理处理\n",
    "神经网络输入层有783个神经元，输出层有10个神经元。\n",
    "其中有两个隐藏层，神经元数量可以设置为任何值。\n",
    "get_data()获取数据\n",
    "init_network()获取权重，偏置参数\n",
    "predict()预测计算\n",
    "predict()函数以NumPy数组的形式输出各个标签对应的概率.\n",
    "np.argmax()函数取出数组中最大值的索引\n",
    "比较神经网络所预测的答案和正确解标签,将正确概率作为识别精度.\n",
    "对神经网络的输入数据 进行某种既定的转换称为预处理(pre-processing)。\n",
    "预处理在神经网络(深度学习)中非常实用，其有效性已在提高识别 性能和学习的效率等众多实验中得到证明。\n",
    "作为一种预处理,我们将各个像素值除以 255，进行了简单的正规化。\n",
    "实际上，很多预处理都会考虑到数据的整体分布。\n",
    "比如，利用数据整体的<font color=#FF0000>**均值**</font>或<font color=#FF0000>**标准差**</font>，<font color=#FF0000>**移动数据**</font>，使<font color=#FF0000>**数据整体以 0 为中心分布**</font>，\n",
    "或者进行正规化，把数据的延展控制在一定范围内。\n",
    "将数据整体的分布形状均匀化的方法，即数据白化(whitening)等。\n",
    "### 批处理\n",
    "\n",
    "查看输入数据和权重参数的形状\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:(10000, 784)\n",
      "x[0]:(784,)\n",
      "W1:(784, 50)\n",
      "W2:(50, 100)\n",
      "W3:(100, 10)\n"
     ]
    }
   ],
   "source": [
    "W1,W2,W3 = network['W1'],network['W2'],network['W3']\n",
    "print('x:'+str(x.shape))\n",
    "print('x[0]:'+str(x[0].shape))\n",
    "print('W1:'+str(W1.shape))\n",
    "print('W2:'+str(W2.shape))\n",
    "print('W3:'+str(W3.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 # 步长，批大小\n",
    "accuracy_cnt = 0\n",
    "print( str(x[0:100].shape))\n",
    "for i in range(0,len(x),batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network,x_batch)\n",
    "    p = np.argmax(y_batch,axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "range(start,end,step) 开始,结束,步长.\n",
    "x[i:i+batch_size],从输入的数据中抽出批数据\n",
    "从第i到i+batch_n个之间的数据。x[0:100]、x[100:200]。\n",
    "argmax()获取值最大元素的索引。参数axis=1。axis可取0或者1。\n",
    "沿着1维的方向找到最大元素的索引.\n",
    "使用NumPy数组(==)生成True、False构成的布尔数组.\n",
    "使用np.sum()方法计算数组中true的个数.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <center>小结\n",
    "- 神经网络中激活函数使用平滑变化的sigmoid函数或者ReLU函数\n",
    "- 可以通过NumPy多维数组,实现高效的神经网络\n",
    "- 机器学习问题分为回归问题和分类问题\n",
    "- 输出层激活函数,回归问题一般使用恒等函数,分类问题中一般使用softmax函数\n",
    "- 分类问题中,输出层的神经元数量设置要为分类的类别数.\n",
    "- 输入数据的集合称为批.通过以批为单位进行推理处理,能够实现高速运算."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}